{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import re\n",
    "from html import unescape\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LabelSmoothingLoss(torch.nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    # 1. Find #[xX]?\\w+; and put '&' to the first\n",
    "    text = re.sub(r'(^|\\D)#\\w+;', lambda match: f'&{match.group()}' if match.group().startswith('#') else f'{match.group()[0]}&{match.group()[1:]}', text)\n",
    "    # 2. Convert HTML character to unicode\n",
    "    text = unescape(text)\n",
    "    # 3. Remove http, https\n",
    "    text = re.sub(r'http\\S+|https\\S+', '', text)\n",
    "    # 4. Remove email\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # 5. Remove twitter id\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # 6. Remove \"&lt;/b&gt;\"\n",
    "    text = re.sub(r'&lt;/b&gt;', '', text)\n",
    "    # 7. Remove &quot; and quot;\n",
    "    text = re.sub(r'&quot;|quot;', '', text)\n",
    "    # 8. Replace &amp; and amp; with &\n",
    "    text = re.sub(r'&amp;|amp;', '&', text)\n",
    "    # 9. Replace &lt; and lt; with <\n",
    "    text = re.sub(r'&lt;|lt;', '<', text)\n",
    "    # 10. Replace &gt; and gt; with >\n",
    "    text = re.sub(r'&gt;|gt;', '>', text)\n",
    "    # 11. Remove the text inside parentheses\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    # 12. Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_texts(texts, tokenizer, max_len=512):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = tokenize_texts(text, self.tokenizer, self.max_len)\n",
    "        return {'input_ids': encoding['input_ids'][0], 'attention_mask': encoding['attention_mask'][0], 'label': torch.tensor(label)}\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Step 1: Set the k_fold\n",
    "k_fold = 1\n",
    "\n",
    "# Step 2: Stratified K-fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "train_val_splits = list(skf.split(train_df, train_df['label']))\n",
    "train_indices, val_indices = train_val_splits[k_fold]\n",
    "\n",
    "# Step 3: Preprocess text\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Save the preprocessed DataFrame as a new CSV file\n",
    "train_df.to_csv('preprocessed_train_data.csv', index=False)\n",
    "\n",
    "# Step 4: Use the selected split\n",
    "train_data = train_df.iloc[train_indices]\n",
    "val_data = train_df.iloc[val_indices]\n",
    "\n",
    "\n",
    "\n",
    "# Load the augmented data\n",
    "sum_aug_df = pd.read_csv('sum_aug.csv')\n",
    "backtrans_aug_df = pd.read_csv('backtrans_aug.csv')\n",
    "\n",
    "# Filter the augmented data by train_df['id']\n",
    "merged_sum_aug_df = sum_aug_df[sum_aug_df['id'].isin(train_df['id'])]\n",
    "merged_backtrans_aug_df = backtrans_aug_df[backtrans_aug_df['id'].isin(train_df['id'])]\n",
    "\n",
    "# Preprocess the text in the merged_sum_aug_df and merged_backtrans_aug_df DataFrames\n",
    "merged_sum_aug_df['text'] = merged_sum_aug_df['text'].apply(preprocess_text)\n",
    "merged_backtrans_aug_df['text'] = merged_backtrans_aug_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Combine the original train_df with the merged augmented data\n",
    "extended_train_df = pd.concat([train_df, merged_sum_aug_df, merged_backtrans_aug_df], ignore_index=True)\n",
    "\n",
    "# Assign the extended_train_df to the train_data\n",
    "train_data = extended_train_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained('sileod/deberta-v3-base-tasksource-nli')\n",
    "\n",
    "train_dataset = NewsDataset(train_data['text'].to_numpy(), train_data['label'].to_numpy(), tokenizer, max_len=512)\n",
    "val_dataset = NewsDataset(val_data['text'].to_numpy(), val_data['label'].to_numpy(), tokenizer, max_len=512)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Step 6: Model, optimizer, and loss function\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('sileod/deberta-v3-base-tasksource-nli', num_labels=8, ignore_mismatched_sizes=True)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Loss function\n",
    "num_classes = 8\n",
    "smoothing = 0.01\n",
    "criterion = LabelSmoothingLoss(classes=num_classes, smoothing=smoothing).to(device)\n",
    "\n",
    "# Step 7: Training and validation\n",
    "def train(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval(model, data_loader, device):\n",
    "  model.eval()\n",
    "  y_true, y_pred = [], []\n",
    "  with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['label'].to(device)\n",
    "      outputs = model(input_ids, attention_mask=attention_mask)\n",
    "      predictions = torch.argmax(outputs.logits, dim=1)\n",
    "      y_true.extend(labels.cpu().numpy())\n",
    "      y_pred.extend(predictions.cpu().numpy())\n",
    "  return y_true, y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_macro_f1 = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "  print(f\"Epoch: {epoch+1}\")\n",
    "  train_loss = train(model, train_loader, optimizer, device)\n",
    "  print(f\"Train Loss: {train_loss}\")\n",
    "  y_true_train, y_pred_train = eval(model, train_loader, device)\n",
    "  y_true_val, y_pred_val = eval(model, val_loader, device)\n",
    "\n",
    "  train_macro_f1 = f1_score(y_true_train, y_pred_train, average='macro')\n",
    "  val_macro_f1 = f1_score(y_true_val, y_pred_val, average='macro')\n",
    "\n",
    "  print(f\"Train Macro F1: {train_macro_f1}\")\n",
    "  print(f\"Val Macro F1: {val_macro_f1}\")\n",
    "  print(classification_report(y_true_val, y_pred_val))\n",
    "\n",
    "  if val_macro_f1 > best_macro_f1:\n",
    "      best_macro_f1 = val_macro_f1\n",
    "      torch.save(model.state_dict(), f'model_{k_fold}.pt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 8: Inference\n",
    "\n",
    "# Load the best model for the current k_fold\n",
    "model.load_state_dict(torch.load(f'model_{k_fold}.pt'))\n",
    "\n",
    "# Create a test dataset with dummy labels (zeros) since we only need the text data for inference\n",
    "test_dataset = NewsDataset(test_df['text'].to_numpy(), np.zeros(len(test_df)), tokenizer, max_len=512)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Perform inference on the test dataset and get the predicted labels\n",
    "_, test_predictions = eval(model, test_loader, device)\n",
    "\n",
    "# Assign the predicted labels to the 'label' column in the test DataFrame\n",
    "test_df['label'] = test_predictions\n",
    "\n",
    "# Save the 'id' and 'label' columns of the test DataFrame to a CSV file\n",
    "test_df[['id', 'label']].to_csv(f'inference_{k_fold}.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'gpt' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n gpt ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Read the selected inference CSV files into separate DataFrames\n",
    "inference_files = [f'inference_{i}.csv' for i in [1, 3]]\n",
    "inference_dfs = [pd.read_csv(file) for file in inference_files]\n",
    "\n",
    "# Extract the 'label' column from each DataFrame\n",
    "label_columns = [df['label'] for df in inference_dfs]\n",
    "\n",
    "# Perform the hard voting for each row using the mode() function\n",
    "voted_labels = mode(label_columns, axis=0).mode[0]\n",
    "\n",
    "# Combine the 'id' column from the first DataFrame with the hard-voted labels into a new DataFrame\n",
    "ensemble_submission = pd.DataFrame({'id': inference_dfs[0]['id'], 'label': voted_labels})\n",
    "\n",
    "# Save the new DataFrame as the final ensemble submission file\n",
    "ensemble_submission.to_csv('ensemble_submission.csv', index=False)\n",
    "print(\"Ensemble submission file saved as 'ensemble_submission.csv'\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Augmentation (summary)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Read the selected inference CSV files into separate DataFrames\n",
    "inference_files = [f'inference_{i}.csv' for i in [1, 3]]\n",
    "inference_dfs = [pd.read_csv(file) for file in inference_files]\n",
    "\n",
    "# Extract the 'label' column from each DataFrame\n",
    "label_columns = [df['label'] for df in inference_dfs]\n",
    "\n",
    "# Perform the hard voting for each row using the mode() function\n",
    "voted_labels = mode(label_columns, axis=0).mode[0]\n",
    "\n",
    "# Combine the 'id' column from the first DataFrame with the hard-voted labels into a new DataFrame\n",
    "ensemble_submission = pd.DataFrame({'id': inference_dfs[0]['id'], 'label': voted_labels})\n",
    "\n",
    "# Save the new DataFrame as the final ensemble submission file\n",
    "ensemble_submission.to_csv('ensemble_submission.csv', index=False)\n",
    "print(\"Ensemble submission file saved as 'ensemble_submission.csv'\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Augmentation (Back Translation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from translatepy import Translate\n",
    "\n",
    "translator = Translate()\n",
    "\n",
    "def back_translate(text: str, target_lang: str = 'ja') -> str:\n",
    "\n",
    "    translated_obj = translator.translate(text, target_lang)\n",
    "    translated = translated_obj.result  # Get the translated text as a string\n",
    "\n",
    "    back_translated_obj = translator.translate(translated, 'en')\n",
    "    back_translated = back_translated_obj.result  # Get the back-translated text as a string\n",
    "\n",
    "    return back_translated\n",
    "        \n",
    "\n",
    "\n",
    "# Load your train.csv\n",
    "data = pd.read_csv('preprocessed_train_data.csv')\n",
    "\n",
    "# Select rows with label 5 or 6\n",
    "selected_data = data[data['label'].isin([5, 6])]\n",
    "\n",
    "\n",
    "\n",
    "# Perform back translation on the 'text' column one by one\n",
    "back_translated_texts = []\n",
    "for text in selected_data['text']:\n",
    "    back_translated_text = back_translate(text, target_lang='ja')\n",
    "    back_translated_texts.append(back_translated_text)\n",
    "    time.sleep(0.1)  # Sleep for 0.1 seconds\n",
    "\n",
    "selected_data['back_translated'] = back_translated_texts\n",
    "\n",
    "# Only keep the back-translated data\n",
    "augmented_data = selected_data[['id', 'back_translated', 'label']].rename(columns={'back_translated': 'text'})\n",
    "\n",
    "# Check if backtrans_aug.csv exists\n",
    "if os.path.exists('backtrans_aug.csv'):\n",
    "    # Read the existing file and append the new data\n",
    "    existing_data = pd.read_csv('backtrans_aug.csv')\n",
    "    combined_data = pd.concat([existing_data, augmented_data], ignore_index=True)\n",
    "else:\n",
    "    # If the file does not exist, create an empty DataFrame with the same columns\n",
    "    combined_data = augmented_data\n",
    "\n",
    "# Save the combined dataset to the CSV file\n",
    "combined_data.to_csv('backtrans_aug.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(additional code) Inference and Ensemble for my model**\n",
    "\n",
    "if you want to inference your data with my fine-tuned model, Run this cell.\n",
    "\n",
    "Prepare your data (test.csv) and model (model_1.pt and model_3.pt)\n",
    "\n",
    "fine-tuned model download : https://www.dropbox.com/sh/80bwc6vlc9gvrse/AAB1t9SXqObr4B82rbJkWHPaa?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    # 1. Find #[xX]?\\w+; and put '&' to the first\n",
    "    text = re.sub(r'(^|\\D)#\\w+;', lambda match: f'&{match.group()}' if match.group().startswith('#') else f'{match.group()[0]}&{match.group()[1:]}', text)\n",
    "    # 2. Convert HTML character to unicode\n",
    "    text = unescape(text)\n",
    "    # 3. Remove http, https\n",
    "    text = re.sub(r'http\\S+|https\\S+', '', text)\n",
    "    # 4. Remove email\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # 5. Remove twitter id\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # 6. Remove \"&lt;/b&gt;\"\n",
    "    text = re.sub(r'&lt;/b&gt;', '', text)\n",
    "    # 7. Remove &quot; and quot;\n",
    "    text = re.sub(r'&quot;|quot;', '', text)\n",
    "    # 8. Replace &amp; and amp; with &\n",
    "    text = re.sub(r'&amp;|amp;', '&', text)\n",
    "    # 9. Replace &lt; and lt; with <\n",
    "    text = re.sub(r'&lt;|lt;', '<', text)\n",
    "    # 10. Replace &gt; and gt; with >\n",
    "    text = re.sub(r'&gt;|gt;', '>', text)\n",
    "    # 11. Remove the text inside parentheses\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    # 12. Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_texts(texts, tokenizer, max_len=512):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = tokenize_texts(text, self.tokenizer, self.max_len)\n",
    "        return {'input_ids': encoding['input_ids'][0], 'attention_mask': encoding['attention_mask'][0], 'label': torch.tensor(label)}\n",
    "\n",
    "\n",
    "\n",
    "def eval(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Preprocess text\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('sileod/deberta-v3-base-tasksource-nli')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('sileod/deberta-v3-base-tasksource-nli', num_labels=8, ignore_mismatched_sizes=True)\n",
    "model.to(device)\n",
    "\n",
    "# Create a test dataset with dummy labels (zeros) since we only need the text data for inference\n",
    "test_dataset = NewsDataset(test_df['text'].to_numpy(), np.zeros(len(test_df)), tokenizer, max_len=512)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Iterate over the desired k_fold values (1 and 3)\n",
    "for k_fold in [1, 3]:\n",
    "    # Load the best model for the current k_fold\n",
    "    model.load_state_dict(torch.load(f'model_{k_fold}.pt'))\n",
    "\n",
    "    # Perform inference on the test dataset and get the predicted labels\n",
    "    _, test_predictions = eval(model, test_loader, device)\n",
    "\n",
    "    # Assign the predicted labels to the 'label' column in the test DataFrame\n",
    "    test_df['label'] = test_predictions\n",
    "\n",
    "    # Save the 'id' and 'label' columns of the test DataFrame to a CSV file\n",
    "    test_df[['id', 'label']].to_csv(f'inference_{k_fold}.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Read the selected inference CSV files into separate DataFrames\n",
    "inference_files = [f'inference_{i}.csv' for i in [1, 3]]\n",
    "inference_dfs = [pd.read_csv(file) for file in inference_files]\n",
    "\n",
    "# Extract the 'label' column from each DataFrame\n",
    "label_columns = [df['label'] for df in inference_dfs]\n",
    "\n",
    "# Perform the hard voting for each row using the mode() function\n",
    "voted_labels = mode(label_columns, axis=0).mode[0]\n",
    "\n",
    "# Combine the 'id' column from the first DataFrame with the hard-voted labels into a new DataFrame\n",
    "ensemble_submission = pd.DataFrame({'id': inference_dfs[0]['id'], 'label': voted_labels})\n",
    "\n",
    "# Save the new DataFrame as the final ensemble submission file\n",
    "ensemble_submission.to_csv('ensemble_submission.csv', index=False)\n",
    "print(\"Ensemble submission file saved as 'ensemble_submission.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
